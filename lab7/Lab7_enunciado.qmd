---
jupyter: python3
---

<h1><center>Laboratorio 7: Interpretabilidad ü§ñ</center></h1>

<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>

### Cuerpo Docente:

- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco
- Auxiliares: Melanie Pe√±a, Valentina Rojas
- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga

### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados

- Nombre de alumno 1: Felipe Hern√°ndez
- Nombre de alumno 2: Brandon Pe√±a

### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/brandonHaipas/MDS7202-Lab-Prog-Ciencia-de-Datos)

### Indice

1. [Temas a tratar](#Temas-a-tratar:)
3. [Descripcci√≥n del laboratorio](#Descripci√≥n-del-laboratorio.)
4. [Desarrollo](#Desarrollo)

## Temas a tratar

- Clasificaci√≥n usando `XGBoost`.
- M√©todos Agn√≥sticos Globales de Interpretabilidad (`Partial Dependence Plot`, `Permutation Feature Importance`)
- M√©todos Agn√≥sticos Locales de Interpretabilidad (`Scoped Rules`, `SHAP`)

## Reglas:

- **Grupos de 2 personas**
- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.
- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.
- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.
- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.
- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.
- Pueden usar cualquier material del curso que estimen conveniente.


### Objetivos principales del laboratorio

- Generar un pipeline de clasificaci√≥n con `XGBoost`.
- Implementar modelos de interpretabilidad para explicar el funcionamiento del modelo de clasificaci√≥n.

El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka "for", "while"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames.

# 1. Problemas Cl√≠nicos del Dr. Simi

<p align="center">
  <img src="https://gantz.cl/wp-content/uploads/2020/01/79024136_2718114448239059_7240913062570491904_o.jpg" width="350">
</p>

El reconocido **Dr. Simi**, famoso vendedor de medicamentos en latinoamerica, debido a la creciente prevalencia de enfermedades cr√≥nicas, como la diabetes, decidi√≥ abrir una cl√≠nica especializada en el tratamiento de esta enfermedad en nuestro pa√≠s.

La cl√≠nica del Doctor Simi se convirti√≥ en un lugar donde los pacientes con diabetes podr√≠an recibir atenci√≥n m√©dica integral. El personal m√©dico estaba compuesto por especialistas en endocrinolog√≠a, nutrici√≥n y enfermer√≠a, todos capacitados en el manejo de la diabetes.

Sin embargo √©l se ha dado cuenta que los tiempos han cambiado y gracias a las tecnolog√≠as es posible generar mejores predicciones en la diabetes conociendo el historial m√©dico de las personas. Por esto, el doctor se ha colocado una meta de incluir modelos de machine learning dentro de sus cl√≠nicas, para ello le ha solicitado crear un modelo capaz de predecir/clasificar diabetes pero le rog√≥ que el desarrollo del modelo tuviera un especial enfoque en la interpretabilidad de lo que hace su modelo.

Para que usted pueda entrenar el modelo, Dr. Simi le ha entregado un dataset de todos los clientes que fueron detectados con diabetes a lo largo de la historia de la clinica. Con ello, adjunta el historial m√©dico de las personas en forma de datos tabulares para que usted pueda realizar f√°cilmente la clasificaci√≥n.

```{python}
#| cell_id: 3252f09552814a80ac2d5e3cc67a1447
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 294
#| execution_start: 1686884002582
#| source_hash: fa5e6129
import numpy as np
import pandas as pd

df = pd.read_csv('diabetes_data.csv')
df.head(5)
```

## 2. Clasificaci√≥n de pacientes con diabetes (5 puntos)

<p align="center">
  <img src="https://media.tenor.com/QH--g3ZaSbsAAAAC/dr-simi-abrazo.gif" width="400">
</p>

Tareas:
1. En primer lugar, el reconocido doctor le pide entrenar un modelo de `XGBoost` utilizando como target la columna `Diabetes` del dataset `diabetes_data.csv`. Para el entrenamiento, realice los siguientes pasos:
  * Realice una breve exploraci√≥n de los datos y determine si aplicar√° transformaciones (MinMaxScaler, StandardScaler, etc.) en alguna/s de las variables. (1 punto)
  * Cree un conjunto de entrenamiento y uno de prueba, con una proporci√≥n de 1/3 en el conjunto de prueba. (0.5 puntos)
  * Cree un ColumnTransformer de preprocesamiento donde aplique las transformaciones determinadas anteriormente. Fije el par√°metro `verbose_feature_names_out=False` y fije la salida del ColumnTransformer en formato pandas mediante el m√©todo `.set_output(transform='pandas')`. (1 punto)
  *  Cree un pipeline donde integre el preprocesamiento y el modelo `XGBoost` y entrene el modelo. Luego utilice `classification_report(..)` para reportar el desempe√±o del modelo. (1 punto)

Comente sus decisiones y los resultados obtenidos con el modelo.

2. Luego, le pide responder las siguientes preguntas:
  *  ¬øEs acaso un buen predictor de diabetes? (0.5 puntos)
  * ¬øQu√© buscan explicar las m√©tricas utilizadas? (0.5 puntos)
  * ¬øLas m√©tricas utilizadas para medir la predictibilidad le permiten asegurar que su modelo haga una buena elecci√≥n de las features?(0.5 puntos)

```{python}
#| cell_id: 85306f784b434ca1a6db2d4751525cf0
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 11299
#| execution_start: 1686884005212
#| source_hash: 4d27a39e
!pip install xgboost
```

Para mantener uniformidad de resultados a trav√©s de las ejecuciones de este notebook, se fijar√° el `random_state` en esta celda bajo el nombre `seed`.

```{python}
seed = 88
```

```{python}
#| cell_id: a928e00142bf45ee8d26207bab5a920d
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 50
#| execution_start: 1686930782578
#| source_hash: 553ccab0
# Exploraci√≥n de datos

df.info()
```

```{python}
df.describe()
```

```{python}
df.shape
```

```{python}
non_binary_cols = ['GenHlth', 'MentHlth','PhysHlth', 'BMI', 'Age']
```

```{python}
binary_cols = []
for col in df.columns:
    if col not in non_binary_cols:
        print(f"valores √∫nicos para la columna {col}: {df[col].nunique()}")
        if col != 'Diabetes':
            binary_cols.append(col)
```

```{python}
df['GenHlth'].value_counts()
```

```{python}
# n√∫mero de valores √∫nicos para las escalas de salud

health_scales = ['MentHlth', 'PhysHlth', 'GenHlth']
for i in health_scales:
    print(f"La escala {i} tiene {df[i].nunique()} valores distintos")
```

```{python}
import plotly.express as xp
# Salud general
hist = xp.histogram(df, x="GenHlth", title="GenHlth distribution")
hist.show()
```

```{python}
# Salud mental?
hist_2 = xp.histogram(df, x="MentHlth", title="MentHlth distribution")
hist_2.show()
```

```{python}
# BMI
hist_3 = xp.histogram(df, x="BMI", title="BMI distribution")
hist_3.show()
```

```{python}
# edad
hist_4 = xp.histogram(df, x="Age", title="Age distribution")
hist_4.show()
```

```{python}
# Salud f√≠sica
hist_5 = xp.histogram(df, x="PhysHlth", title="PhysHlth distribution")
hist_5.show()
```

De esta exploraci√≥n podemos notar que `Age`, `BMI`, `GenHlth`, `MentHlth` y `PhysHlth` son variables num√©ricas no binarias. Podemos decir que en todas estas variables tiene sentido que haya un orden, puesto que corresponden a n√∫meros en una escala o derechamente a una edad. Se aplicar√° un `RobustScaler` para lidiar con los outliers de `MentHlth`, `BMI` y `PhysHlth`; se aplicar√° un `MinMaxScaler` sobre `Age` y `GenHlth` por su distribuci√≥n m√°s regular y tambi√©n sobre los datos binarios puesto que sus valores no cambiar√°n.

```{python}
# Column Transformer de preprocesamiento
from sklearn.preprocessing import RobustScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer

preprocessing = ColumnTransformer(
    transformers = [
            (
                "MinMax Scaler",
                MinMaxScaler(),
                ["Age", "GenHlth"] + binary_cols
            ),
            (
                "Robust Scaler",
                RobustScaler(),
                ["MentHlth", "BMI", "PhysHlth"]
            )
        ],
    verbose_feature_names_out=False
)

preprocessing.set_output(transform='pandas')
```

```{python}
# split de datos en entrenamiento y test
from sklearn.model_selection import train_test_split
y = df['Diabetes']
X = df.drop('Diabetes', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state= seed)
```

```{python}
# pipeline
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier

xgb_pipeline = Pipeline(
   steps = [
            ("Preprocesamiento", preprocessing),
            ("XGBClassifier", XGBClassifier())
        ]
)
```

```{python}
# Entrenamiento
xgb_pipeline.fit(X_train, y_train)

# Evaluaci√≥n
from sklearn.metrics import classification_report
y_pred_baseline = xgb_pipeline.predict(X_test)
print(f'Evaluaci√≥n de xgboost sin finetunning\n{classification_report(y_test, y_pred_baseline)}')
```

## 3. Importancia de las features con XGBoost (5 puntos)

<p align="center">
  <img src="https://media.tenor.com/5JAj5_IiagEAAAAd/dr-simi-dr-simi-dance.gif" width="400">
</p>

Tareas:
1. Para a√±adir el toque de interpretabilidad que Dr. Simi le pide, se le pide calcular la **importancia de las features** del modelo entrenado utilizando todos los m√©todos (*weight*, *cover*, *gain*) que posee xgboost usando `plot_importance`. `Hint:` Puede acceder a un paso de un pipeline por su nombre mediante el m√©todo `.named_steps[...]` (3 puntos)
2. ¬øLos resultados obtenidos con los diferentes m√©todos son compatibles?, comente sus resultados y a que se debe la igualdad o desigualdad que ve en los resultados. (1 punto)
3. Finalmente, ¬ølas importancias obtenidas son suficientes para obtener la interpretabilidad de un modelo que utiliza √°rboles? ¬øQu√© debilidad presenta este m√©todo? (1 punto)

```{python}
#| cell_id: 32be9645ac03408b98705d22def41265
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 2
#| execution_start: 1686930815998
#| source_hash: 553ccab0
# Obtenci√≥n del clasificador

xgb_clf = xgb_pipeline.named_steps['XGBClassifier']
```

```{python}
# plot_importance con weight

import xgboost as xgb
import matplotlib.pyplot as plt

# weight: numbero de veces que una feature es usada para dividir la data
xgb.plot_importance(xgb_clf, max_num_features=17, importance_type="weight")
plt.title("Feature Importance con importance_type='weight'")
plt.show()
```

```{python}
# plot_importance con weight

import xgboost as xgb
import matplotlib.pyplot as plt

# weight: numbero de veces que una feature es usada para dividir la data
xgb.plot_importance(xgb_clf, max_num_features=17, importance_type="cover")
plt.title("Feature Importance con importance_type='cover'")
plt.show()
```

```{python}
# plot_importance con weight

import xgboost as xgb
import matplotlib.pyplot as plt

# weight: numbero de veces que una feature es usada para dividir la data
xgb.plot_importance(xgb_clf, max_num_features=17, importance_type="gain")
plt.title("Feature Importance con importance_type='gain'")
plt.show()
```

## 4. M√©todos Agn√≥sticos Globales (10 puntos)

<p align="center">
  <img src="https://media.tenor.com/JcRHtjVuXN8AAAAC/dr-simi-farmacias-similares.gif" width="400">
</p>

Tareas:
1. Para mitigar los problemas encontrados en la secci√≥n anterior, Dr. Simi le pide implementar un **m√©todo de permutaci√≥n** que le permita observar la importancia de las features. `Nota:`Tenga cuidado con el orden de las columnas de este m√©todo. `Hint:` Puede obtener los features del clasificador con su respectivo orden mediante el m√©todo `.get_booster().feature_names` (2 puntos)
2. Para que su modelo sea consistente, repita el proceso **30 veces** y verifique la desviaci√≥n est√°ndar de sus resultados (¬øQu√© se√±ala esta?). (2 puntos)
3. Visualice los resultados de este m√©todo en un gr√°fico. (2 puntos)
4. Adem√°s, responda las siguientes preguntas:
  - ¬øC√≥mo mide la importancia de las features su propuesta? (1 punto)
  - ¬øQu√© features tienen un mayor impacto en la salida del modelo?. Comente las 5 primeras, ¬øtienen sentido? (1 punto)
  - ¬øC√≥mo cambian sus conclusiones con respecto a las features importances del punto anterior? (1 punto)
  - Nombre y explique 3 ventajas y 3 desventajas del m√©todo implementado. (1 punto)

```{python}
#| cell_id: 83032f5e2894423dbafa952e1d3f89cb
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 43036
#| execution_start: 1686882223141
#| source_hash: c8f2357f
# Permutation feature importance
import numpy as np
from sklearn.metrics import accuracy_score, mean_squared_error, f1_score

# recibe un df de pandas X, una serie y, una metrica, un random state
def custom_permutation_importance(clf, X, y, metric=mean_squared_error, n_times=10, random_state=seed, importance_cal = 'diff'):
    if importance_cal not in ['diff', 'div']:
        raise Exception('Invalid argument for importance type')
    baseline_score = metric(clf.predict(X), y)
    rng = np.random.RandomState(random_state)
    # features
    features = clf.get_booster().feature_names
    raw_result = np.zeros(shape=(len(features), n_times))
    # repeat n times
    for i in range(0, n_times):
        idx = 0
        for col in features:
            X_perm = X.copy(deep=True)
            #shuffle the column
            shuffled = X_perm[col].sample(frac=1, random_state = rng.randint(0, n_times*100)).reset_index(drop=True)
            X_perm[col] = shuffled
            perm_score = metric(y, clf.predict(X_perm))
            importance = None
            if importance_cal == 'diff':
                importance = baseline_score - perm_score
            elif importance_cal == 'div':
                importance = perm_score/baseline_score
            if importance is None:
                raise Exception("Unexpected behaviour")
            raw_result[idx, i] += importance
            idx +=1

    return {"importance_mean": raw_result.mean(axis=1), "importance_std": raw_result.std(axis=1), "importances":raw_result}   
```

```{python}
subpipeline = Pipeline(xgb_pipeline.steps[:-1])
processed_x_test = subpipeline.transform(X_test)
importances_output = custom_permutation_importance(xgb_clf, processed_x_test, y_test, n_times=30, metric = f1_score)
```

```{python}
perm_sorted_idx = importances_output['importance_mean'].argsort()
plt.boxplot(
    importances_output['importances'][perm_sorted_idx].T,
    vert=False,
    labels=np.array(xgb_clf.get_booster().feature_names)[perm_sorted_idx],
)
plt.title("Importancia de las Features por Incremento del Error")
plt.xlabel('Error')
plt.show()
```

## 5. M√©todos Agn√≥sticos Locales (20 puntos)

<p align="center">
  <img src="https://i.makeagif.com/media/10-24-2024/oMCrLI.gif" width="400">
</p>

### 5.1 Calculando Shap Values (4 puntos)

Tareas:
1. Alegre por saber c√≥mo funciona el modelo de predicci√≥n a nivel general, Dr. Simi le pide ahora interpretar las predicciones de su modelo a nivel de paciente (es decir, desde un punto de vista **local**). Para esto, el ilustre farmac√©utico le pide calcular los *shap values* de su modelo. (2 puntos)
2. ¬øQu√© representa cada n√∫mero en su resultado? (1 punto)
3. ¬øEs posible atribuir un significado a la positividad/negatividad de cada valor? (1 punto)

```{python}
#| cell_id: dada61becc854fe4830665a1e1fcfb8b
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 7982
#| execution_start: 1686925464848
#| source_hash: 5d97bad
!pip install shap
```

```{python}
#| cell_id: c2d01e18bb9946579275006ac63f29f8
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 2
#| execution_start: 1686884837621
#| source_hash: 7220fd12
import warnings
warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")
```

```{python}
#| cell_id: 28773926243a4632adf4ebe7f9b23c86
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 3
#| execution_start: 1686930931899
#| source_hash: 7b093107
import shap

explainer = shap.TreeExplainer(xgb_clf)
# cambiar X por el 'x' de mi dataset completo
X = pd.concat([X_train, X_test])
processed_X = subpipeline.transform(X)
shap_values = explainer(processed_X)
```

### 5.2 Aporte local (4 puntos)

1. Usando los *shap values* calculados, grafique el **aporte local** de las diferentes variables para las instancias **1**, **9** y **150** (1 punto).

2. Interprete sus resultados y responda:

  - ¬øQu√© variables afectan de manera positiva/negativa a la probabilidad de poseer diabetes? (1 punto)

  - ¬øExiste alg√∫n patr√≥n com√∫n entre las instancias analizadas? (1 punto)

  - ¬øEs posible generalizar estas conclusiones a todo el dataset? (1 punto)

```{python}
#| cell_id: 114ead8d9d674348a6eb462b83e54f0b
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 2
#| execution_start: 1686930940135
#| source_hash: 5f8720eb
idx = 1

shap.initjs()
shap.plots.waterfall(shap_values[idx,:], 
                     max_display=14)
```

```{python}
idx = 9

shap.initjs()
shap.plots.waterfall(shap_values[idx,:], 
                     max_display=14)
```

```{python}
idx = 150

shap.initjs()
shap.plots.waterfall(shap_values[idx,:], 
                     max_display=14)
```

### 5.3 Aporte global (4 puntos)

Genere ahora una visualizaci√≥n donde se grafique el aporte de cada feature a nivel **global** e interprete sus resultados. ¬øQu√© diferencias existen con las conclusiones generadas a nivel de instancia?

```{python}
#| cell_id: 54e88d349b7c476d82a95b977ce23fd4
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 4
#| execution_start: 1686930956967
#| source_hash: 99e7a96a
shap.summary_plot(shap_values, processed_X, plot_type="bar")
```

### 5.4 Scatter plot (4 puntos)

Grafique ahora un *scatterplot* entre los *shap values* y las primeras 5 features con mayor impacto global (un gr√°fico por cada feature), coloreando cada punto por la probabilidad de tener diabetes. ¬øQu√© puede concluir de sus resultados?

```{python}
#| cell_id: fe980bf5ee9c4cda84cb87db6e6f41dd
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 2
#| execution_start: 1686930965534
#| source_hash: 80c98595
# obtenemos los shap values
shap_values_abs = np.mean(np.abs(shap_values.values), axis=0)

# Obtenemos los nombres ordenados de mayor a menor
feature_importance_names = processed_X.columns[shap_values_abs.argsort()[::-1]]
```

```{python}
# Revisamos las features
feature_importance_names[:17]
```

```{python}
predictions = xgb_clf.predict(processed_X)

for name in feature_importance_names[:5]:
    shap.plots.scatter(shap_values[:,name], color=predictions)
```

### 5.5 Partial Dependence Plot (4 puntos)

Finalmente, se le pide generar un gr√°fico del tipo Partial Dependence Plot para las mismas 5 variables con mayor impacto global usando una submuestra de 1000 observaciones. ¬øQu√© relaci√≥n existe entre la salida promedio del modelo y cada feature analizada? ¬øSon estas conclusiones generalizables para todo el conjunto de datos?

```{python}
#| cell_id: fe7a7b2e6f664d129fee9f64dd57f6bd
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 4
#| execution_start: 1686930977674
#| source_hash: 4e0f176b
X1000 = shap.utils.sample(processed_X, 1000)

sample_ind = 20
shap.partial_dependence_plot(
    'GenHlth', xgb_clf.predict, X1000, model_expected_value=True,
    feature_expected_value=True, ice=True,
    shap_values=shap_values[sample_ind:sample_ind+1,:],
)
```

```{python}
X1000 = shap.utils.sample(processed_X, 1000)

sample_ind = 20
shap.partial_dependence_plot(
    'HighBP', xgb_clf.predict, X1000, model_expected_value=True,
    feature_expected_value=True, ice=True,
    shap_values=shap_values[sample_ind:sample_ind+1,:],
)
```

```{python}
X1000 = shap.utils.sample(processed_X, 1000)

sample_ind = 20
shap.partial_dependence_plot(
    'BMI', xgb_clf.predict, X1000, model_expected_value=True,
    feature_expected_value=True, ice=True,
    shap_values=shap_values[sample_ind:sample_ind+1,:],
)
```

```{python}
X1000 = shap.utils.sample(processed_X, 1000)

sample_ind = 20
shap.partial_dependence_plot(
    'Age', xgb_clf.predict, X1000, model_expected_value=True,
    feature_expected_value=True, ice=True,
    shap_values=shap_values[sample_ind:sample_ind+1,:],
)
```

```{python}
X1000 = shap.utils.sample(processed_X, 1000)

sample_ind = 20
shap.partial_dependence_plot(
    'HighChol', xgb_clf.predict, X1000, model_expected_value=True,
    feature_expected_value=True, ice=True,
    shap_values=shap_values[sample_ind:sample_ind+1,:],
)
```

## 6. Sistema de Reglas! (10 punto)

<p align="center">
  <img src="https://media.baamboozle.com/uploads/images/125978/1638281150_1380186_gif-url.gif" width="400">
</p>

Despu√©s de todo el trabajo hecho, Dr. Simi le pide simplificar el funcionamiento de su modelo en un sistema de reglas que le permita explicar a sus clientes las predicciones que genera su modelo.
En particular, Dr. Simi le pide explicar la decisi√≥n tomada para las observaciones **1000**, **3001** y **5751**. Con las reglas propuestas se√±ale a **cu√°nta poblaci√≥n** es posible explicar con estas reglas e indique la **precisi√≥n** que poseen las reglas en la totalidad de los datos. ¬øTienen sentido sus reglas propuestas para las observaciones?. Fundamente sus respuesta se√±alando el impacto que tienen sus reglas sobre todo el conjunto de datos.

`Hint:` Como debe entregar las columnas que entran al clasificador entrenado de su pipeline, le ser√° √∫til extraer el paso de preprocesamiento y generar dataframes preprocesados para el conjunto `train` y `test`.

```{python}
#| cell_id: 9e3327e11a104dd5917f594a5e10baee
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 21693
#| execution_start: 1686885552805
#| source_hash: e7a6b6c4
#| tags: []
!pip install alibi
```

```{python}
#| cell_id: bb38bb05376b429297fb0b3618fec6d4
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 10473
#| execution_start: 1686885694481
#| source_hash: e4ec2c4b
from alibi.explainers import AnchorTabular
```

```{python}
#| cell_id: c51b7d6bfea74288b77cb73ba8c86978
#| deepnote_cell_type: code
#| deepnote_to_be_reexecuted: false
#| execution_millis: 76
#| execution_start: 1686931515362
#| source_hash: 51992da3
# Inserte su c√≥digo para generar sistema de reglas aqu√≠
processed_x_train = subpipeline.transform(X_train)
predict_fn = lambda x: xgb_clf.predict(x) # creamos funci√≥n de predicci√≥n
explainer = AnchorTabular(predict_fn, xgb_clf.feature_names_in_, seed=seed) # instanciamos Anchor usando funci√≥n creada
explainer.fit(processed_x_train.to_numpy()) # sobre los datos

idx = 1000
# explicar predicci√≥n idx 1000
obs = np.array(processed_X.iloc[idx], ndmin = 2) # array de la obs

# generar anchor
explanation = explainer.explain(obs, # obs a explicar
                                threshold=0.90, # min precision de anchor
                                )
print(f"Index {idx}")
print('Prediction: ', explainer.predictor(processed_X.to_numpy()[idx].reshape(1, -1))[0]) # obtener prediccion del modelo
print('Anchor: %s' % (' AND '.join(explanation.anchor))) # obtener regla
print('Precision: %.2f' % explanation.precision) # obtener precision
print('Coverage: %.2f' % explanation.coverage, '\n') # obtener cobertura

idx = 3001
# explicar predicci√≥n idx 3001
bs = np.array(processed_X.iloc[idx], ndmin = 2) # array de la obs

# generar anchor
explanation = explainer.explain(obs, # obs a explicar
                                threshold=0.90, # min precision de anchor
                                )
print(f"Index {idx}")
print('Prediction: ', explainer.predictor(processed_X.to_numpy()[idx].reshape(1, -1))[0]) # obtener prediccion del modelo
print('Anchor: %s' % (' AND '.join(explanation.anchor))) # obtener regla
print('Precision: %.2f' % explanation.precision) # obtener precision
print('Coverage: %.2f' % explanation.coverage, '\n') # obtener cobertura

idx = 5751
# explicar predicci√≥n idx 5751
bs = np.array(processed_X.iloc[idx], ndmin = 2) # array de la obs

# generar anchor
explanation = explainer.explain(obs, # obs a explicar
                                threshold=0.90, # min precision de anchor
                                )
print(f"Index {idx}")
print('Prediction: ', explainer.predictor(processed_X.to_numpy()[idx].reshape(1, -1))[0]) # obtener prediccion del modelo
print('Anchor: %s' % (' AND '.join(explanation.anchor))) # obtener regla
print('Precision: %.2f' % explanation.precision) # obtener precision
print('Coverage: %.2f' % explanation.coverage) # obtener cobertura
```

# Retrospectiva... (10 puntos)

En base a los diferentes m√©todos que implementa y ha comentado en este laboratorio, comente qu√© m√©todos le permiten entregar mejores conclusiones para la tarea de clasificaci√≥n de diabetes. Por otro lado, ¬øqu√© m√©todos son m√°s √∫tiles para el problema del doctor Simi, m√©todos agnosticos locales o globales?

> Fundamente su Respuesta aqu√≠

<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>

