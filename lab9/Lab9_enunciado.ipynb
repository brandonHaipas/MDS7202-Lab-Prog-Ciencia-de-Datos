{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl0JoW4Eodvi"
      },
      "source": [
        "# **Laboratorio 9: Airflow 🛫**\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos - Otoño 2025</strong></center>\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Stefano Schiappacasse, Sebastián Tinoco\n",
        "- Auxiliares: Melanie Peña, Valentina Rojas\n",
        "- Ayudantes: Angelo Muñoz, Valentina Zúñiga"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ypG7Fsodvj"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n",
        "\n",
        "- Nombre de alumno 1: Felipe Hernández\n",
        "- Nombre de alumno 2: Brandon Peña\n",
        "\n",
        "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/brandonHaipas/MDS7202-Lab-Prog-Ciencia-de-Datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_P7PCPTodvk"
      },
      "source": [
        "## Temas a tratar\n",
        "\n",
        "- Construcción de pipelines productivos usando `Airflow`.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 6 días de plazo con descuento de 1 punto por día. Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia será debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no estén en u-cursos no serán revisados. Recuerden que el repositorio también tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Reconocer los componentes pricipales de `Airflow` y su funcionamiento.\n",
        "- Poner en práctica la construcción de pipelines de `Airflow`.\n",
        "- Automatizar procesos típicos de un proyecto de ciencia de datos mediante `Airflow` y `Docker`.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsfK981Uodvk"
      },
      "source": [
        "# **Introducción**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ilM8YDjodvk"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/OBQ6niqbxswAAAAM/legallyblonde.gif\" width=\"300\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrLPQNBodvk"
      },
      "source": [
        "Vale, una estudiante del Magíster en Ciencia de Datos, se encuentra en la etapa final de sus estudios. Por un lado, está muy contenta por haber llegado tan lejos, pero por otro, no puede evitar sentirse inquieta. Desde que ingresó a la universidad, una pregunta la ha perseguido: ¿qué tan probable es que pueda ser seleccionada en los lugares donde envíe postulaciones para puestos de trabajo?\n",
        "\n",
        "Esta duda la mantiene en constante reflexión, especialmente porque sabe que el mercado laboral en Ciencia de Datos es competitivo y exige habilidades no solo técnicas, sino también estratégicas para destacar. Sin embargo, Vale actualmente está completamente enfocada en terminar su tesis de magíster y ha tenido que postergar cualquier preparación específica para enfrentar el desafío de las postulaciones laborales.\n",
        "\n",
        "Al ver el avance y las habilidades que usted ha demostrado en el curso, Vale decidió proponerle un desafío que le permitirá disminuir la incertidumbre sobre su futuro laboral. Inspirado en sus conocimientos, recolectó un conjunto de datos que contiene información sobre diversos factores que influyen en las decisiones de contratación de empresas al seleccionar entre sus postulantes. Este set de datos incluye los siguientes atributos:\n",
        "\n",
        "- Age: Edad del candidato\n",
        "- Gender: Genero del candidato. Male (0), Female (1).\n",
        "- EducationLevel: Mayor nivel educacional alcanzado por el candidato. Licenciatura Tipo 1 (1), Licenciatura Tipo 2 (2), Maestría (3), PhD. (4).\n",
        "- ExperienceYears: Años de experiencia profesional.\n",
        "- PreviousCompanies: Numero de compañías donde el candidato ha trabajado anteriormente.\n",
        "- DistanceFromCompany: Distancia en kilometros entre la residencia del candidato y la compañía donde postula.\n",
        "- InterviewScore: Puntaje obtenido en la entrevista por el candidato entre 0 a 100.\n",
        "- SkillScore: Puntaje obtenido en evaluación de habilidades técnicas por el candidato, entre 0 a 100.\n",
        "- PersonalityScore: Puntaje obtenido en pruebas de personalidad del candidato, entre 0 a 100.\n",
        "- RecruitmentStrategy: Estrategia del equipo de reclutamiento. Agresiva (1), Moderada (2), Conservadora (3).\n",
        "\n",
        "Variable a predecir:\n",
        "- HiringDecision: Resultado de la postulación. No contratado (0), Contratado (1).\n",
        "\n",
        "Su objetivo será ayudar a Vale a desarrollar un modelo que le permita predecir, basado en estos factores, si un postulante será contratado o no. Esta herramienta no solo le dará a Vale mayor claridad sobre el impacto de ciertos atributos en la decisión final de contratación, sino que también le permitirá aplicar sus conocimientos de Ciencia de Datos para resolver una pregunta que a muchos estudiantes como a ella les inquieta.\n",
        "\n",
        "Como estudiante del curso Laboratorio de Programación Científica para Ciencia de Datos, deberá demostrar sus capacidades para preprocesar, analizar y modelar datos, brindándole a Vale una solución robusta y bien fundamentada para su problemática.\n",
        "\n",
        "`Nota:` El siguiente [enlace](https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data/data) contiene el set de datos original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yeh268atodvl"
      },
      "source": [
        "# **1. Pipeline de Predicción Lineal** (30 Puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmB1LTWnodvl"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.licdn.com/dms/image/v2/D4E22AQHZplrdPyKnvA/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1713736729086?e=2147483647&v=beta&t=Tad2ulaWkhhDrPRN0PCdXrfuza60PjoJqgLborDyLao\" width=\"500\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF1bTY0Modvl"
      },
      "source": [
        "En esta sección buscaremos desplegar un producto utilizando un modelo de clasificación `Random Forest` para determinar **si una persona será contratada o no en un proceso de selección**. Para ello, comenzaremos preparando un pipeline lineal mediante `Airflow`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7MllF4fodvl"
      },
      "source": [
        "## **1.1 Preparando el Pipeline** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1JxaZgModvl"
      },
      "source": [
        "**Primero, asegúrese de tener creada las carpetas `dags`, `plugins` y `logs`**.\n",
        "\n",
        "Comenzamos preparando un archivo llamado `hiring_functions.py`, el cual guardará en la carpeta `dags` y debe contener lo siguiente:\n",
        "\n",
        "1. (3 puntos) Una función llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecución como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
        "  - raw\n",
        "  - splits\n",
        "  - models\n",
        "\n",
        "  `Hint`: Puede hacer uso de kwargs para obtener la fecha de ejecución mediante el DAG. El siguiente [Enlace](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html) le puede ser útil.\n",
        "\n",
        "2. (3 puntos) Una función llamada `split_data()` que lea el archivo `data_1.csv` de la carepta `raw` y a partir de este, aplique un *hold out*, generando un dataset de entrenamiento y uno de prueba. Luego debe guardar estos nuevos conjuntos de datos en la carpeta `splits`. `Nota:` Utilice un 20% para el conjunto de prueba, mantenga la proporción original en la variable objetivo y fije una semilla.\n",
        "\n",
        "3. (8 puntos) Cree una función llamada `preprocess_and_train()` que:\n",
        "  - Lea los set de entrenamiento y prueba de la carpeta `splits`.\n",
        "  - Cree y aplique un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes. Puede apoyarse del archivo `data_1_report.html` para justificar cualquier paso del preprocesamiento.\n",
        "  \n",
        "  - Añada una etapa de entrenamiento utilizando el modelo `RandomForest`.\n",
        "  \n",
        "  Esta función **debe crear un archivo `joblib` (análogo a `pickle`) con el pipeline entrenado** en la carepta `models`, además debe **imprimir** el accuracy en el conjunto de prueba y el f1-score de la clase positiva (contratado).\n",
        "3. (1 punto) Incorpore la función `gradio_interface` en su script, modificando la ruta de acceso a su modelo, de forma que pueda leerlo desde la carepta `models`. Puede realizar las modificaciones que estime necesarias.\n",
        "\n",
        "`NOTA:` Se permite la creación de funciones auxiliares si lo estiman conveniente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hiring_functions.py\n",
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import gradio as gr\n",
        "\n",
        "# Seed Aleatoria\n",
        "seed = 123\n",
        "\n",
        "# Variable home\n",
        "home_dir = os.getenv('AIRFLOW_HOME')\n",
        "\n",
        "def create_folders(**kwargs):\n",
        "    date = kwargs.get(\"ds\")\n",
        "    os.mkdir(date)\n",
        "    os.mkdir(f\"{home_dir}/{date}/raw\")\n",
        "    os.mkdir(f\"{home_dir}/{date}/splits\")\n",
        "    os.mkdir(f\"{home_dir}/{date}/models\")\n",
        "    return\n",
        "\n",
        "def split_data(**kwargs):\n",
        "    dir = f\"{kwargs.get('ds')}\"\n",
        "    df = pd.read_csv(f\"{home_dir}/{dir}/raw/data_1.csv\")\n",
        "    target_col = \"HiringDecision\"\n",
        "    X = df.drop(columns=[target_col])\n",
        "    X_cols = X.columns\n",
        "    y = df[target_col]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "    \n",
        "    train_df = pd.DataFrame(X_train, columns=X_cols)\n",
        "    train_df[target_col] = y_train\n",
        "\n",
        "    test_df = pd.DataFrame(X_test, columns=X_cols)\n",
        "    test_df[target_col] = y_test\n",
        "\n",
        "    train_df.to_csv(f\"{home_dir}/{dir}/splits/train.csv\", index=False)\n",
        "    test_df.to_csv(f\"{home_dir}/{dir}/splits/test.csv\", index=False)\n",
        "\n",
        "class TypeTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, type_to_transform):\n",
        "        self.type_to_transform = type_to_transform\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return X.astype(self.type_to_transform)\n",
        "    \n",
        "    def set_output(self, transform=None):\n",
        "        self._transform_output = transform\n",
        "        return self\n",
        "\n",
        "def preprocess_and_train(**kwargs):\n",
        "    dir = f\"{kwargs.get('ds')}\"\n",
        "    train_df = pd.read_csv(f\"{home_dir}/{dir}/splits/train.csv\")\n",
        "    test_df = pd.read_csv(f\"{home_dir}/{dir}/splits/test.csv\")\n",
        "\n",
        "    # basandose en data_1_report, se puede concluir que las unicas variables que necesitan one_hot_encoding son EducationLevel RecruitmentStrategy.\n",
        "    # el resto de columnas \"categoricas\" solo necesita un cambio de tipos.\n",
        "    # para el resto de variables se podría aplicar un scaler, dado que no hay un mayor desbalance se puede usar directamente un minmax scaler.\n",
        "\n",
        "    encode_cols = [\"EducationLevel\", \"RecruitmentStrategy\"]\n",
        "    astype_cols = [\"PreviousCompanies\"]\n",
        "    minmax_cols = [\"Age\", \"ExperienceYears\", \"PreviousCompanies\", \"DistanceFromCompany\", \"InterviewScore\", \"SkillScore\"]\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    type_transformer_int = TypeTransformer(type_to_transform=int)\n",
        "    type_transformer_str = TypeTransformer(type_to_transform=str)\n",
        "\n",
        "    type_transformer = ColumnTransformer([\n",
        "        (\"Type Transformer Categorical\", type_transformer_str, encode_cols),\n",
        "        (\"Type Transformer Numerical\", type_transformer_int, astype_cols),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False)\n",
        "\n",
        "    encoder_transformer = ColumnTransformer([\n",
        "        (\"One Hot Encoding\", encoder, encode_cols),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False)\n",
        "\n",
        "    scaler_transformer = ColumnTransformer([\n",
        "        (\"MinMax Scaler\", scaler, minmax_cols)\n",
        "    ], \n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False)\n",
        "    \n",
        "    rf = RandomForestClassifier()\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"Type\", type_transformer),\n",
        "        (\"Encoder\", encoder_transformer),\n",
        "        (\"Scaler\", scaler_transformer),\n",
        "        (\"Random Forest Classifier\", rf)\n",
        "    ]).set_output(transform=\"pandas\")\n",
        "\n",
        "    X = train_df.drop(columns=[\"HiringDecision\"])\n",
        "    y = train_df[\"HiringDecision\"]\n",
        "    pipeline.fit(X, y)\n",
        "\n",
        "    joblib.dump(pipeline, f\"{home_dir}/{dir}/models/pipeline.joblib\")\n",
        "    X_test = test_df.drop(columns=[\"HiringDecision\"])\n",
        "    y_test = test_df[\"HiringDecision\"]\n",
        "    predictions = pipeline.predict(X_test)\n",
        "\n",
        "    report = classification_report(y_true=y_test, y_pred=predictions,output_dict=True)\n",
        "    accuracy = report[\"accuracy\"]\n",
        "    f1_score = report[\"1\"][\"f1-score\"]\n",
        "\n",
        "    print(f\"Accuracy for positive class: {accuracy:.2f}\")\n",
        "    print(f\"F1-Score for positive class: {f1_score:.2f}\")\n",
        "\n",
        "def predict(file, model_path):\n",
        "    pipeline = joblib.load(model_path)\n",
        "    input_data = pd.read_json(file)\n",
        "    predictions = pipeline.predict(input_data)\n",
        "    print(f'La prediccion es: {predictions}')\n",
        "    labels = [\"No contratado\" if pred == 0 else \"Contratado\" for pred in predictions]\n",
        "\n",
        "    return {'Predicción': labels[0]}\n",
        "\n",
        "def gradio_interface(**kwargs):\n",
        "    dir = f\"{kwargs.get('ds')}\"\n",
        "    model_path= f\"{home_dir}/{dir}/models/pipeline.joblib\" # Completar con la ruta del modelo entrenado\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=lambda file: predict(file, model_path),\n",
        "        inputs=gr.File(label=\"Sube un archivo JSON\"),\n",
        "        outputs=\"json\",\n",
        "        title=\"Hiring Decision Prediction\",\n",
        "        description=\"Sube un archivo JSON con las características de entrada para predecir si Vale será contratada o no.\"\n",
        "    )\n",
        "    interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTKOj1hfodvm"
      },
      "source": [
        "## **1.2 Creando Nuestro DAG** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkEZcEh4odvm"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/a_yibuZQgngAAAAM/elle-woods.gif\" width=\"400\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-MTaxTgodvm"
      },
      "source": [
        "Con las funciones del pipeline ya creadas, ahora vamos a proceder a crear un Directed Acyclic Graph (DAG). Para ello, se le pide lo siguiente:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-yUak2Rodvm"
      },
      "source": [
        "- (10 puntos) Cree un segundo archivo llamado `dag_lineal.py` y guardelo en la carpeta dags. Este script debe seguir la siguiente estructura (Ver imagen de referencia):\n",
        "\n",
        "    0. Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, ejecución manual y **sin backfill**. Asigne un `dag_id` que pueda reconocer facilmente, como `hiring_lineal`, etc.\n",
        "    1. Debe comenzar con un marcador de posición que indique el inicio del pipeline.\n",
        "    2. Cree una carpeta correspondiente a la ejecución del pipeline y cree las subcarpetas `raw`, `splits` y `models` mediante la función `create_folders()`.\n",
        "    3. Debe descargar el archivo `data_1.csv` del siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv). Debe guardar el archivo en la carpeta raw de la ejecución correspondiente.`Hint:` Le puede ser útil el comando `curl -o <path de guardado> <enlace con los datos>`.\n",
        "    4. Debe aplicar un hold out mediante la función `split_data()` de su archivo creado en la subsección anterior.\n",
        "    5. Debe aplicar el preprocesamiento y el entrenamiento del modelo mediante la función `preprocess_and_train()`.\n",
        "    6. Finalmente, debe montar una interfaz en gradio donde pueda cargar un archivo ``json``.\n",
        "\n",
        "\n",
        "- (3 puntos) Cree un `DockerFile` para montar un contenedor que contenga Airflow. Adicionalmente, cree una carpeta llamada dags donde guardará el script.py creado anteriormente.\n",
        "\n",
        "    `Nota:` Para la imagen, se recomienda utilizar python 3.10-slim. Adicionalmente, puede instalar `curl` mediante la siguiente linea de código: `RUN apt-get update && apt-get install -y curl`.\n",
        "\n",
        "- Construya el contenedor en Docker y acceda a la aplicación web de Airflow mediante el siguiente [enlace](http://localhost:8080/). Inicie sesión, acceda al DAG creado y ejecute de forma manual su pipeline.\n",
        "\n",
        "- (2 puntos) Acceda a la URL pública de Gradio e ingrese el archivo `vale_data.json` a su modelo. ¿Que predicción entregó el modelo para Vale? Adjunte imágenes de su resultado. `Hint:` Puede acceder a los `logs` para obtener los prints y la URL pública.\n",
        "\n",
        "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecución `ds`.\n",
        "\n",
        "**Para esta sección, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que serán ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar imágenes de apoyo, como screenshots.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiMTgQfJpuIv"
      },
      "source": [
        "DAG de referencia:\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/uc?id=1iwDgECZfFeWq1dl433tMa6_3CNF9cn1L\" width=\"1200\">\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckzDqsF4odvn"
      },
      "outputs": [],
      "source": [
        "# dag_lineal.py\n",
        "from datetime import datetime, date\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "\n",
        "from hiring_functions import create_folders, split_data, preprocess_and_train, gradio_interface\n",
        "\n",
        "with DAG(\n",
        "    dag_id = \"hiring_lineal\",\n",
        "    description=\"A lineal dag\",\n",
        "    start_date = datetime(2024,10,1),\n",
        "    catchup=False,\n",
        "    schedule=None\n",
        ") as dag:\n",
        "    \n",
        "    marker_task = EmptyOperator(task_id=\"Starting_the_process\", retries =2)\n",
        "\n",
        "    folder_task = PythonOperator(\n",
        "        task_id=\"Creating_folders\",\n",
        "        python_callable = create_folders\n",
        "    )\n",
        "\n",
        "    download_dataset_task = BashOperator(\n",
        "        task_id='Download_dataset',\n",
        "        bash_command='curl -o $AIRFLOW_HOME/{{ ds }}/raw/data_1.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv'\n",
        "    )\n",
        "\n",
        "    holdout_task = PythonOperator(\n",
        "        task_id=\"Holdout\",\n",
        "        python_callable= split_data\n",
        "    )\n",
        "\n",
        "    preprocess_and_train_task = PythonOperator(\n",
        "        task_id=\"Preprocess_and_train\",\n",
        "        python_callable=preprocess_and_train\n",
        "    )\n",
        "\n",
        "    gradio_gui_task = PythonOperator(\n",
        "        task_id=\"Gradio_GUI\",\n",
        "        python_callable=gradio_interface\n",
        "    )\n",
        "\n",
        "    # pipeline definition\n",
        "    marker_task >> folder_task >> download_dataset_task >> holdout_task >> preprocess_and_train_task >> gradio_gui_task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "dockerfile"
        }
      },
      "outputs": [],
      "source": [
        "# Utiliza una imagen base con Python instalado\n",
        "FROM python:3.10-slim\n",
        "\n",
        "# Establece el directorio de trabajo en el contenedor\n",
        "WORKDIR /root/airflow\n",
        "\n",
        "# Establece la variable de entorno AIRFLOW_HOME\n",
        "ENV AIRFLOW_HOME=/root/airflow\n",
        "\n",
        "# Desactiva los DAGs de ejemplo\n",
        "ENV AIRFLOW__CORE__LOAD_EXAMPLES=False\n",
        "\n",
        "# Instalar dependencias del sistema\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    libgomp1 \\\n",
        "    curl \\\n",
        "    && apt-get clean \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Instala Apache Airflow\n",
        "RUN pip install \"apache-airflow[password,sqlite]==2.8.1\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.8.1/constraints-3.10.txt\"\n",
        "RUN pip install scikit-learn\n",
        "RUN pip install pandas\n",
        "RUN pip install numpy\n",
        "RUN pip install joblib\n",
        "RUN pip install gradio\n",
        "RUN pip install xgboost\n",
        "\n",
        "# Inicializa la base de datos de Airflow\n",
        "RUN airflow db migrate\n",
        "\n",
        "# Expone el puerto 8080 para el servidor web de Airflow\n",
        "EXPOSE 8080\n",
        "\n",
        "# Crea el usuario admin de Airflow\n",
        "RUN airflow users create --role Admin --username admin --email admin \\\n",
        " --firstname admin --lastname admin --password admin\n",
        "\n",
        "# Copia las carpetas necesarias al contenedor\n",
        "COPY ./dags $AIRFLOW_HOME/dags\n",
        "COPY ./logs $AIRFLOW_HOME/logs\n",
        "COPY ./plugins $AIRFLOW_HOME/plugins\n",
        "\n",
        "# Comando para iniciar el servidor web y el scheduler\n",
        "CMD [\"sh\", \"-c\", \"airflow webserver -p 8080 & airflow scheduler\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resultados\n",
        "El modelo de Vale arrojó la predicción de `\"Contratado\"` en Gradio.\n",
        "\n",
        "![](img/gradio.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqBlHcBQpXJb"
      },
      "source": [
        "# **2. Paralelizando el Pipeline** (30 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.gifer.com/8LNL.gif\" width=\"400\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoQaVOeiqO_R"
      },
      "source": [
        "Al ver los resultados obtenidos, Vale queda muy contenta con el clasificador. Sin embargo, le aparecen algunas dudas respecto al funcionamiento del pipeline. Primero le comenta que es posible que en un futuro tenga nuevos datos que podrían ser útiles para realizar nuevos entrenamientos, por lo que sería ideal si este pipeline se fuera ejecutando de forma periódica y **NO** de forma manual. Además, Vale le menciona que le gustaría explorar el desempeño de otros modelos además de `Random Forest`, de forma que el pipeline seleccione de forma automática el modelo con mejor desempeño para luego hacer la predicción de Vale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mGPMg0ur-wR"
      },
      "source": [
        "## **2.1 Preparando un Nuevo Pipeline** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpU81VCRr-Hr"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/gnA7-5TewXMAAAAM/elle-woods.gif\" width=\"400\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KcXuS6bsZAw"
      },
      "source": [
        "De acuerdo a lo que le comentó Vale, usted decide crear un nuevo script con las funciones que utilizará su pipeline. Por ende, dentro de la carpeta `dags`, usted creará el archivo `hiring_dynamic_functions.py` el cual debe contener:\n",
        "\n",
        "1. (2 puntos) Una función llamada `create_folders()` que cree una carpeta, la cual utilice la fecha de ejecución como nombre. Adicionalmente, dentro de esta carpeta debe crear las siguientes subcarpetas:\n",
        "  - raw\n",
        "  - preprocessed\n",
        "  - splits\n",
        "  - models\n",
        "2. (2 puntos) Una función llamada `load_ands_merge()` que lea desde la carpeta `raw` los archivos `data_1.csv`y `data_2.csv` en caso de estar disponible. Luego concatene estos y genere un nuevo archivo resultante, guardándolo en la carpeta `preprocessed`.\n",
        "\n",
        "3. (2 puntos) Una función llamada `split_data()` que lea la data guardada en la carpeta `preprocessed` y realice un hold out sobre esta data. Esta función debe crear un conjunto de entrenamiento y uno de prueba. Mantenga una semilla y 20% para el conjunto de prueba. Guarde los conjuntos resultantes en la carpeta `splits`.\n",
        "\n",
        "4. (6 puntos) Una función llamada `train_model()` que reciba un modelo de clasificación.\n",
        "    - La función debe comenzar leyendo el conjunto de entrenamiento desde la carpeta `spits`.\n",
        "    - Esta debe crear y aplicar un `Pipeline` con una etapa de preprocesamiento. Utilice `ColumnTransformers` para aplicar las transformaciones que estime convenientes.\n",
        "    - Añada una etapa de entrenamiento utilizando un modelo que ingrese a la función.\n",
        "  \n",
        "  Esta función **debe crear un archivo joblib con el pipeline entrenado**. Guarde el modelo con un nombre que le permita una facil identificación dentro de la carpeta `models`.\n",
        "\n",
        "5. (3 puntos) Una función llamada `evaluate_models()` que reciba sus modelos entrenados desde la carpeta `models`, evalúe su desempeño mediante `accuracy` en el conjunto de prueba y seleccione el mejor modelo obtenido. Luego guarde el mejor modelo como archivo `.joblib`. Su función debe imprimir el nombre del modelo seleccionado y el accuracy obtenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnX61hxjW9rI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Seed Aleatoria\n",
        "seed = 123\n",
        "\n",
        "# Variable home\n",
        "home_dir = os.getenv('AIRFLOW_HOME')\n",
        "\n",
        "def create_folders(**kwargs):\n",
        "    date = kwargs.get(\"ds\")\n",
        "    os.mkdir(date)\n",
        "    os.mkdir(f\"{home_dir}/{date}/raw\")\n",
        "    os.mkdir(f\"{home_dir}/{date}/splits\")\n",
        "    os.mkdir(f\"{home_dir}/{date}/models\")\n",
        "    os.mkdir(f\"{home_dir}/{date}/preprocessed\")\n",
        "    return\n",
        "\n",
        "def load_and_merge(**kwargs):\n",
        "    dir = f\"{kwargs.get('ds')}\"\n",
        "    df_1 = pd.read_csv(f\"{home_dir}/{dir}/raw/data_1.csv\")\n",
        "    df_2 = None\n",
        "    new_df = None\n",
        "    try:\n",
        "        df_2 = pd.read_csv(f\"{home_dir}/{dir}/raw/data_2.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"No data_2.csv file found, proceeding without it\")\n",
        "    if df_2 is None:\n",
        "        new_df = df_1\n",
        "    else:\n",
        "        df_2 = df_2[df_1.columns.to_list()] # mismo orden de columnas\n",
        "        new_df = pd.concat([df_1, df_2], ignore_index=True)\n",
        "    new_df.to_csv(f\"{home_dir}/{dir}/preprocessed/prep_data.csv\")\n",
        "    return\n",
        "\n",
        "def split_data(**kwargs):\n",
        "    dir = f\"{kwargs.get('ds')}\"\n",
        "    df = pd.read_csv(f\"{home_dir}/{dir}/preprocessed/prep_data.csv\")\n",
        "    target_col = \"HiringDecision\"\n",
        "    X = df.drop(columns=[target_col])\n",
        "    X_cols = X.columns\n",
        "    y = df[target_col]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "    \n",
        "    train_df = pd.DataFrame(X_train, columns=X_cols)\n",
        "    train_df[target_col] = y_train\n",
        "\n",
        "    test_df = pd.DataFrame(X_test, columns=X_cols)\n",
        "    test_df[target_col] = y_test\n",
        "\n",
        "    train_df.to_csv(f\"{home_dir}/{dir}/splits/train.csv\", index=False)\n",
        "    test_df.to_csv(f\"{home_dir}/{dir}/splits/test.csv\", index=False)\n",
        "\n",
        "class TypeTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, type_to_transform):\n",
        "        self.type_to_transform = type_to_transform\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return X.astype(self.type_to_transform)\n",
        "    \n",
        "    def set_output(self, transform=None):\n",
        "        self._transform_output = transform\n",
        "        return self\n",
        "\n",
        "def train_model(model, model_name, **kwargs):\n",
        "    dir = f\"{kwargs.get('ds')}\"\n",
        "    ti = kwargs['ti']\n",
        "    train_df = pd.read_csv(f\"{home_dir}/{dir}/splits/train.csv\")\n",
        "\n",
        "    # basandose en data_1_report, se puede concluir que las unicas variables que necesitan one_hot_encoding son EducationLevel RecruitmentStrategy.\n",
        "    # el resto de columnas \"categoricas\" solo necesita un cambio de tipos.\n",
        "    # para el resto de variables se podría aplicar un scaler, dado que no hay un mayor desbalance se puede usar directamente un minmax scaler.\n",
        "\n",
        "    encode_cols = [\"EducationLevel\", \"RecruitmentStrategy\"]\n",
        "    astype_cols = [\"PreviousCompanies\"]\n",
        "    minmax_cols = [\"Age\", \"ExperienceYears\", \"PreviousCompanies\", \"DistanceFromCompany\", \"InterviewScore\", \"SkillScore\"]\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    type_transformer_int = TypeTransformer(type_to_transform=int)\n",
        "    type_transformer_str = TypeTransformer(type_to_transform=str)\n",
        "\n",
        "    type_transformer = ColumnTransformer([\n",
        "        (\"Type Transformer Categorical\", type_transformer_str, encode_cols),\n",
        "        (\"Type Transformer Numerical\", type_transformer_int, astype_cols),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False)\n",
        "\n",
        "    encoder_transformer = ColumnTransformer([\n",
        "        (\"One Hot Encoding\", encoder, encode_cols),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False)\n",
        "\n",
        "    scaler_transformer = ColumnTransformer([\n",
        "        (\"MinMax Scaler\", scaler, minmax_cols)\n",
        "    ], \n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False)\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"Type\", type_transformer),\n",
        "        (\"Encoder\", encoder_transformer),\n",
        "        (\"Scaler\", scaler_transformer),\n",
        "        (\"Classifier\", model)\n",
        "    ]).set_output(transform=\"pandas\")\n",
        "\n",
        "    X_train = train_df.drop(columns=[\"HiringDecision\"])\n",
        "    y_train = train_df[\"HiringDecision\"]\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    model_path = f\"{home_dir}/{dir}/models/{model_name}_pipeline.joblib\"\n",
        "    joblib.dump(pipeline, model_path)\n",
        "    ti.xcom_push(key=f\"{model_name}_model\", value={'model_path': model_path, 'model_name': model_name})\n",
        "\n",
        "def evaluate_models(**kwargs):\n",
        "    dir = f\"{kwargs.get('ds')}\"\n",
        "    ti = kwargs['ti']\n",
        "\n",
        "    test_df = pd.read_csv(f\"{home_dir}/{dir}/splits/test.csv\")\n",
        "    X_test = test_df.drop(columns=[\"HiringDecision\"])\n",
        "    y_test = test_df[\"HiringDecision\"]\n",
        "\n",
        "    models = {\n",
        "        'rf': ti.xcom_pull(key='rf_model', task_ids='Training_rf'),\n",
        "        'xgb': ti.xcom_pull(key='xgb_model', task_ids='Training_xgb'),\n",
        "        'et': ti.xcom_pull(key='et_model', task_ids='Training_et')\n",
        "    }\n",
        "\n",
        "    model_paths = {\n",
        "        'rf': models['rf']['model_path'],\n",
        "        'xgb': models['xgb']['model_path'],\n",
        "        'et': models['et']['model_path'],\n",
        "    }\n",
        "\n",
        "    best_score = 0\n",
        "    best_name = \"\"\n",
        "    best = None\n",
        "    accuracy_dict={}\n",
        "    for key in model_paths.keys():\n",
        "        model_path = model_paths[key]\n",
        "        pipeline = joblib.load(model_path)\n",
        "        predictions = pipeline.predict(X_test)\n",
        "        score = accuracy_score(y_true=y_test, y_pred=predictions)\n",
        "        accuracy_dict[key]=score\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_name = key\n",
        "            best = pipeline\n",
        "    \n",
        "    for key in accuracy_dict.keys():\n",
        "        ti.xcom_push(key=f\"{key}_accuracy\", value={'model_accuracy': accuracy_dict[key]})\n",
        "\n",
        "    joblib.dump(best, f\"{home_dir}/{dir}/models/best_pipeline.joblib\")\n",
        "    print(f\"The best model is {best_name} with an accuracy of {accuracy_dict[best_name]:.2f}!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUYkXWcZJz3b"
      },
      "source": [
        "## **2.2 Componiendo un nuevo DAG** (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak7uL9YXJ6Xj"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://67.media.tumblr.com/bfa5208006dc3f404ec08e8c3195cf2c/tumblr_obg9tgnLfX1u9e9f2o2_r1_500.gif\" width=\"500\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbE6mu20LfWd"
      },
      "source": [
        "Con las nuevas funciones, se debe crear el nuevo nuevo DAG. Para ello, cree un nuevo script en la carpeta `dags`, llamandolo `dag_dynamic.py`. Este script debe contener la siguiente estructura:\n",
        "\n",
        "1. (1 punto) Inicialice un DAG con fecha de inicio el 1 de octubre de 2024, el cual se debe ejecutar el día 5 de cada mes a las 15:00 UTC. Utilice un `dag_id` interpretable para identificar fácilmente. **Habilite el backfill** para que pueda ejecutar tareas programadas desde fechas pasadas.\n",
        "2. (1 punto) Comience con un marcador de posición que indique el inicio del pipeline.\n",
        "3. (2 puntos) Cree una carpeta correspondiente a la ejecución del pipeline y cree las subcarpetas `raw`, `preprocessed`, `splits` y `models` mediante la función `create_folders()`.\n",
        "4. (2 puntos) Implemente un `Branching`que siga la siguiente lógica:\n",
        "  - Fechas previas al 1 de noviembre de 2024: Se descarga solo `data_1.csv`\n",
        "  - Desde el 1 de noviembre del 2024: descarga `data_1.csv` y `data_2.csv`.\n",
        "  En el siguiente [enlace](https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv) puede descargar `data_2.csv`.\n",
        "5. (1 punto) Cree una tarea que concatene los datasets disponibles mediante la función `load_and_merge()`. Configure un `Trigger` para que la tarea se ejecute si encuentra disponible **como mínimo** uno de los archivos.\n",
        "6. (1 punto) Aplique el hold out al dataset mediante la función `split_data()`, obteniendo un conjunto de entrenamiento y uno de prueba.\n",
        "7. (2 puntos) Realice 3 entrenamientos en paralelo:\n",
        "  - Un modelo Random Forest.\n",
        "  - 2 modelos a elección.\n",
        "  Asegúrese de guardar sus modelos entrenados con nombres distintivos. Utilice su función `train_model()` para ello.\n",
        "8. (2 puntos) Mediante la función `evaluate_models()`, evalúe los modelos entrenados, registrando el accuracy de cada modelo en el set de prueba. Luego debe imprimir el mejor modelo seleccionado y su respectiva métrica. Configure un `Trigger` para que la tarea se ejecute solamente si los 3 modelos fueron entrenados y guardados.\n",
        "\n",
        "`Hint:` Recuerde que puede entregar `kwargs` a sus funciones, como por ejemplo la fecha de ejecución `ds`.\n",
        "\n",
        "Una vez creado el script, vuelva a construir el contenedor en Docker, acceda a la aplicación web de Airflow, ejecute su pipeline y muestre sus resultados. Adjunte imágenes que ayuden a mostrar el proceso y sus resultados.\n",
        "\n",
        "Adicionalmente, responda (1 c/u):\n",
        "\n",
        "- ¿Cual es el accuracy de cada modelo en la ejecución de octubre? ¿Se obtienen los mismos resultados a partir de Noviembre?\n",
        "- Analice como afectó el añadir datos a sus modelos mediante el desempeño del modelo y en costo computacional.\n",
        "- Muestre el esquema de su DAG ejecutado en octubre y en noviembre.\n",
        "\n",
        "\n",
        "`Nota:` Para esta sección no debe implementar la tarea en gradio, solamente se espera determinar el mejor modelo y comparar el desempeño obtenido.\n",
        "\n",
        "**IMPORTANTE: Para esta sección, debe adjuntar todos los scripts creados junto a su notebook en la entrega, ya que serán ejecutados para validar el funcionamiento. Para justificar sus respuestas, adicionaslmente puede utilizar imágenes de apoyo, como screenshots.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMgK2sKTYJji"
      },
      "outputs": [],
      "source": [
        "#dags/dag_dynamic.py\n",
        "from datetime import datetime, date\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.python import BranchPythonOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from hiring_dynamic_functions import create_folders, load_and_merge, split_data, train_model, evaluate_models, seed\n",
        "\n",
        "dag =  DAG(\n",
        "    dag_id = \"hiring_dynamic\",\n",
        "    description=\"A dynamic dag\",\n",
        "    start_date = datetime(2024,10,1),\n",
        "    catchup=True,\n",
        "    schedule_interval='0 15 5 * *'\n",
        ") \n",
        "\n",
        "# Task 1 - Print start statement\n",
        "start_task = EmptyOperator(task_id=\"Starting_the_process\", retries=2)\n",
        "\n",
        "\n",
        "# Task 2 - Folder creation operator\n",
        "folder_task = PythonOperator(\n",
        "    task_id=\"Creating_folders\",\n",
        "    python_callable = create_folders,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "def branch_by_date(**kwargs):\n",
        "    execution_date = kwargs['logical_date'].date()\n",
        "    threshold_date = date(2024, 11, 1)\n",
        "\n",
        "    if execution_date < threshold_date:\n",
        "        return 'Download_dataset_1'\n",
        "    else:\n",
        "        return 'Download_both_datasets'\n",
        "\n",
        "# Task 3 - Branch operator\n",
        "date_branching_task = BranchPythonOperator(\n",
        "    task_id= \"Date_branching\",\n",
        "    python_callable=branch_by_date,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Task 3.a - Download data_1.csv\n",
        "download_dataset_1_task = BashOperator(\n",
        "    task_id='Download_dataset_1',\n",
        "    bash_command='curl -o $AIRFLOW_HOME/{{ ds }}/raw/data_1.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Task 3.b - Download both datasets\n",
        "download_dataset_1_and_2_task = BashOperator(\n",
        "    task_id = 'Download_both_datasets',\n",
        "    bash_command='curl -o $AIRFLOW_HOME/{{ ds }}/raw/data_1.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_1.csv -o $AIRFLOW_HOME/{{ ds }}/raw/data_2.csv https://gitlab.com/eduardomoyab/laboratorio-13/-/raw/main/files/data_2.csv',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Task 4 - Load and merge\n",
        "load_and_merge_task = PythonOperator(\n",
        "    task_id = \"Load_and_merge\",\n",
        "    python_callable=load_and_merge,\n",
        "    trigger_rule ='one_success',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Task 5 - Holdout\n",
        "holdout_task = PythonOperator(\n",
        "    task_id=\"Holdout\",\n",
        "    python_callable= split_data,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Task 6.a - Train Random Forest\n",
        "train_rf_task = PythonOperator(\n",
        "    task_id=\"Training_rf\",\n",
        "    python_callable = train_model,\n",
        "    op_kwargs = {\"model_name\": \"rf\", \"model\": RandomForestClassifier(random_state = seed)},\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "# Task 6.a - Train XGBoost\n",
        "train_xgb_task = PythonOperator(\n",
        "    task_id='Training_xgb',\n",
        "    python_callable = train_model,\n",
        "    op_kwargs = {\"model_name\": \"xgb\", \"model\": XGBClassifier(random_state =seed)},\n",
        "    dag= dag\n",
        ")\n",
        "\n",
        "# Task 6.a - Train Extra Tree\n",
        "train_et_task = PythonOperator(\n",
        "    task_id='Training_et',\n",
        "    python_callable = train_model,\n",
        "    op_kwargs = {\"model_name\": \"et\", \"model\": ExtraTreesClassifier(random_state = seed)},\n",
        "    dag = dag\n",
        ")\n",
        "\n",
        "# Task 7 - Evaluate\n",
        "evaluate_task = PythonOperator(\n",
        "    task_id='Evaluate_models',\n",
        "    python_callable = evaluate_models,\n",
        "    dag=dag,\n",
        "    trigger_rule='all_success'\n",
        ")\n",
        "\n",
        "# pipeline definition\n",
        "start_task >> folder_task >> date_branching_task\n",
        "date_branching_task >> [download_dataset_1_task, download_dataset_1_and_2_task]\n",
        "download_dataset_1_task >> load_and_merge_task\n",
        "download_dataset_1_and_2_task >> load_and_merge_task\n",
        "load_and_merge_task >> holdout_task >> [train_rf_task, train_xgb_task, train_et_task] >> evaluate_task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación se puede ver la ejecución del contenedor de docker en la terminal, así como la interfaz de Airflow con la ejecución del DAG hiring_dynamic:\n",
        "\n",
        "![](img/terminal.png)\n",
        "\n",
        "![](img/execution.png)\n",
        "\n",
        "**Respuestas**:\n",
        "- _¿Cual es el accuracy de cada modelo en la ejecución de octubre? ¿Se obtienen los mismos resultados a partir de Noviembre?_ R: En la ejecución de octubre se obtiene un accuracy de 0.85 para Random Forest, 0.89 para XGBoost y 0.81 para Extra Trees. A partir de noviembre, Random Forest y XGBoost obtienen 0.91 y Extra Trees 0.87. Es decir, en el primer período XGBoost es el mejor modelo indiscutidamente, pero a partir del mes próximo hay un empate entre XGBoost y Random Forest. Todos los modelos mejoran su accuracy a partir de noviembre.\n",
        "\n",
        "Accuracies para Octubre:\n",
        "\n",
        "![](img/accuracy_october.png)\n",
        "\n",
        "Accuracies para Noviembre en adelante:\n",
        "\n",
        "![](img/accuracy_november_onwards.png)\n",
        "\n",
        "Cabe notar que la fecha de ejecución sobre los screenshots corresponde al fin del período mensual, es decir, que para los datos descargados durante período que empezó a ejecutarse un mes atrás de la fecha en cuestión, esas son las **accuracies**.\n",
        "\n",
        "- _Analice como afectó el añadir datos a sus modelos mediante el desempeño del modelo y en costo computacional._ En terminos de desempeño, ya se mostró que los modelos mejoran su accuracy al incorporar más datos. Respecto al costo computacional, tiene sentido pensar que al haber más datos que procesar se requieran más recursos computacionales. En efecto, el tiempo de ejecución en octubre del 2024 fue de 1 minuto y 10 segundos, mientras que para los meses posteriores osciló entre 1 minuto 25 segundos y 1 minuto 47 segundos, como se puede ver a continuación:\n",
        "\n",
        "![](img/durations.png)\n",
        "- _Muestre el esquema de su DAG ejecutado en octubre y en noviembre._\n",
        "\n",
        "Octubre:\n",
        "![](img/octubre-dag.png)\n",
        "\n",
        "Noviembre:\n",
        "![](img/noviembre-dag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrmM65RIRrgm"
      },
      "source": [
        "# Conclusión\n",
        "\n",
        "Éxito!\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:1000/1*PX8WVijZapo7EDrvGv9Inw.gif\" width=\"500\">\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Lab_MDS_Primavera",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
